{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RowNumber    CustomerId   CreditScore           Age        Tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800      5.012800   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806      2.892174   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000      0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000      3.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000      5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000     10.000000   \n",
       "\n",
       "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
       "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
       "mean    76485.889288       1.530200      0.70550        0.515100   \n",
       "std     62397.405202       0.581654      0.45584        0.499797   \n",
       "min         0.000000       1.000000      0.00000        0.000000   \n",
       "25%         0.000000       1.000000      0.00000        0.000000   \n",
       "50%     97198.540000       1.000000      1.00000        1.000000   \n",
       "75%    127644.240000       2.000000      1.00000        1.000000   \n",
       "max    250898.090000       4.000000      1.00000        1.000000   \n",
       "\n",
       "       EstimatedSalary        Exited  \n",
       "count     10000.000000  10000.000000  \n",
       "mean     100090.239881      0.203700  \n",
       "std       57510.492818      0.402769  \n",
       "min          11.580000      0.000000  \n",
       "25%       51002.110000      0.000000  \n",
       "50%      100193.915000      0.000000  \n",
       "75%      149388.247500      0.000000  \n",
       "max      199992.480000      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RowNumber</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>-0.006495</td>\n",
       "      <td>-0.009067</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>-0.005988</td>\n",
       "      <td>-0.016571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerId</th>\n",
       "      <td>0.004202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>-0.014025</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>-0.006248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditScore</th>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003965</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.027094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>-0.003965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009997</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>-0.030680</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>0.085472</td>\n",
       "      <td>-0.007201</td>\n",
       "      <td>0.285323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>-0.006495</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>-0.009997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012254</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.028362</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>-0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>-0.009067</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.028308</td>\n",
       "      <td>-0.012254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.304180</td>\n",
       "      <td>-0.014858</td>\n",
       "      <td>-0.010084</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.118533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumOfProducts</th>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>-0.030680</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>-0.304180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>-0.047820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasCrCard</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.014025</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.014858</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011866</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>-0.007138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsActiveMember</th>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>0.085472</td>\n",
       "      <td>-0.028362</td>\n",
       "      <td>-0.010084</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>-0.011866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011421</td>\n",
       "      <td>-0.156128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <td>-0.005988</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.007201</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>-0.011421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exited</th>\n",
       "      <td>-0.016571</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>-0.027094</td>\n",
       "      <td>0.285323</td>\n",
       "      <td>-0.014001</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>-0.047820</td>\n",
       "      <td>-0.007138</td>\n",
       "      <td>-0.156128</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RowNumber  CustomerId  CreditScore       Age    Tenure  \\\n",
       "RowNumber         1.000000    0.004202     0.005840  0.000783 -0.006495   \n",
       "CustomerId        0.004202    1.000000     0.005308  0.009497 -0.014883   \n",
       "CreditScore       0.005840    0.005308     1.000000 -0.003965  0.000842   \n",
       "Age               0.000783    0.009497    -0.003965  1.000000 -0.009997   \n",
       "Tenure           -0.006495   -0.014883     0.000842 -0.009997  1.000000   \n",
       "Balance          -0.009067   -0.012419     0.006268  0.028308 -0.012254   \n",
       "NumOfProducts     0.007246    0.016972     0.012238 -0.030680  0.013444   \n",
       "HasCrCard         0.000599   -0.014025    -0.005458 -0.011721  0.022583   \n",
       "IsActiveMember    0.012044    0.001665     0.025651  0.085472 -0.028362   \n",
       "EstimatedSalary  -0.005988    0.015271    -0.001384 -0.007201  0.007784   \n",
       "Exited           -0.016571   -0.006248    -0.027094  0.285323 -0.014001   \n",
       "\n",
       "                  Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "RowNumber       -0.009067       0.007246   0.000599        0.012044   \n",
       "CustomerId      -0.012419       0.016972  -0.014025        0.001665   \n",
       "CreditScore      0.006268       0.012238  -0.005458        0.025651   \n",
       "Age              0.028308      -0.030680  -0.011721        0.085472   \n",
       "Tenure          -0.012254       0.013444   0.022583       -0.028362   \n",
       "Balance          1.000000      -0.304180  -0.014858       -0.010084   \n",
       "NumOfProducts   -0.304180       1.000000   0.003183        0.009612   \n",
       "HasCrCard       -0.014858       0.003183   1.000000       -0.011866   \n",
       "IsActiveMember  -0.010084       0.009612  -0.011866        1.000000   \n",
       "EstimatedSalary  0.012797       0.014204  -0.009933       -0.011421   \n",
       "Exited           0.118533      -0.047820  -0.007138       -0.156128   \n",
       "\n",
       "                 EstimatedSalary    Exited  \n",
       "RowNumber              -0.005988 -0.016571  \n",
       "CustomerId              0.015271 -0.006248  \n",
       "CreditScore            -0.001384 -0.027094  \n",
       "Age                    -0.007201  0.285323  \n",
       "Tenure                  0.007784 -0.014001  \n",
       "Balance                 0.012797  0.118533  \n",
       "NumOfProducts           0.014204 -0.047820  \n",
       "HasCrCard              -0.009933 -0.007138  \n",
       "IsActiveMember         -0.011421 -0.156128  \n",
       "EstimatedSalary         1.000000  0.012097  \n",
       "Exited                  0.012097  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels? (Total number of positive cases)\n",
    "data['Exited'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname:\n",
      "2932\n",
      "['Hargrave' 'Hill' 'Onio' ... 'Kashiwagi' 'Aldridge' 'Burbidge']\n",
      "\n",
      "Geography:\n",
      "3\n",
      "['France' 'Spain' 'Germany']\n",
      "\n",
      "Gender:\n",
      "2\n",
      "['Female' 'Male']\n"
     ]
    }
   ],
   "source": [
    "# Number and names of categorical entries\n",
    "print(\"Surname:\")\n",
    "print(data['Surname'].nunique())\n",
    "print(data['Surname'].unique())\n",
    "print()\n",
    "print(\"Geography:\")\n",
    "print(data['Geography'].nunique())\n",
    "print(data['Geography'].unique())\n",
    "print()\n",
    "print(\"Gender:\")\n",
    "print(data['Gender'].nunique())\n",
    "print(data['Gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8000 (0%)]\tLoss: 0.584051\n",
      "Train Epoch: 0 [1400/8000 (17%)]\tLoss: 0.348156\n",
      "Train Epoch: 0 [2800/8000 (35%)]\tLoss: 0.363388\n",
      "Train Epoch: 0 [4200/8000 (52%)]\tLoss: 0.485302\n",
      "Train Epoch: 0 [5600/8000 (70%)]\tLoss: 0.446140\n",
      "Train Epoch: 0 [7000/8000 (87%)]\tLoss: 0.520859\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.387808\n",
      "Train Epoch: 1 [1400/8000 (17%)]\tLoss: 0.322274\n",
      "Train Epoch: 1 [2800/8000 (35%)]\tLoss: 0.364290\n",
      "Train Epoch: 1 [4200/8000 (52%)]\tLoss: 0.486450\n",
      "Train Epoch: 1 [5600/8000 (70%)]\tLoss: 0.447236\n",
      "Train Epoch: 1 [7000/8000 (87%)]\tLoss: 0.521105\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.387712\n",
      "Train Epoch: 2 [1400/8000 (17%)]\tLoss: 0.322828\n",
      "Train Epoch: 2 [2800/8000 (35%)]\tLoss: 0.364308\n",
      "Train Epoch: 2 [4200/8000 (52%)]\tLoss: 0.486650\n",
      "Train Epoch: 2 [5600/8000 (70%)]\tLoss: 0.447443\n",
      "Train Epoch: 2 [7000/8000 (87%)]\tLoss: 0.521223\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.387688\n",
      "Train Epoch: 3 [1400/8000 (17%)]\tLoss: 0.323025\n",
      "Train Epoch: 3 [2800/8000 (35%)]\tLoss: 0.364315\n",
      "Train Epoch: 3 [4200/8000 (52%)]\tLoss: 0.486721\n",
      "Train Epoch: 3 [5600/8000 (70%)]\tLoss: 0.447524\n",
      "Train Epoch: 3 [7000/8000 (87%)]\tLoss: 0.521273\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.387678\n",
      "Train Epoch: 4 [1400/8000 (17%)]\tLoss: 0.323114\n",
      "Train Epoch: 4 [2800/8000 (35%)]\tLoss: 0.364318\n",
      "Train Epoch: 4 [4200/8000 (52%)]\tLoss: 0.486754\n",
      "Train Epoch: 4 [5600/8000 (70%)]\tLoss: 0.447563\n",
      "Train Epoch: 4 [7000/8000 (87%)]\tLoss: 0.521298\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.387674\n",
      "Train Epoch: 5 [1400/8000 (17%)]\tLoss: 0.323159\n",
      "Train Epoch: 5 [2800/8000 (35%)]\tLoss: 0.364320\n",
      "Train Epoch: 5 [4200/8000 (52%)]\tLoss: 0.486770\n",
      "Train Epoch: 5 [5600/8000 (70%)]\tLoss: 0.447583\n",
      "Train Epoch: 5 [7000/8000 (87%)]\tLoss: 0.521311\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.387672\n",
      "Train Epoch: 6 [1400/8000 (17%)]\tLoss: 0.323182\n",
      "Train Epoch: 6 [2800/8000 (35%)]\tLoss: 0.364321\n",
      "Train Epoch: 6 [4200/8000 (52%)]\tLoss: 0.486779\n",
      "Train Epoch: 6 [5600/8000 (70%)]\tLoss: 0.447594\n",
      "Train Epoch: 6 [7000/8000 (87%)]\tLoss: 0.521317\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.387670\n",
      "Train Epoch: 7 [1400/8000 (17%)]\tLoss: 0.323195\n",
      "Train Epoch: 7 [2800/8000 (35%)]\tLoss: 0.364322\n",
      "Train Epoch: 7 [4200/8000 (52%)]\tLoss: 0.486783\n",
      "Train Epoch: 7 [5600/8000 (70%)]\tLoss: 0.447600\n",
      "Train Epoch: 7 [7000/8000 (87%)]\tLoss: 0.521321\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.387670\n",
      "Train Epoch: 8 [1400/8000 (17%)]\tLoss: 0.323202\n",
      "Train Epoch: 8 [2800/8000 (35%)]\tLoss: 0.364322\n",
      "Train Epoch: 8 [4200/8000 (52%)]\tLoss: 0.486786\n",
      "Train Epoch: 8 [5600/8000 (70%)]\tLoss: 0.447604\n",
      "Train Epoch: 8 [7000/8000 (87%)]\tLoss: 0.521323\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 9 [1400/8000 (17%)]\tLoss: 0.323206\n",
      "Train Epoch: 9 [2800/8000 (35%)]\tLoss: 0.364322\n",
      "Train Epoch: 9 [4200/8000 (52%)]\tLoss: 0.486787\n",
      "Train Epoch: 9 [5600/8000 (70%)]\tLoss: 0.447605\n",
      "Train Epoch: 9 [7000/8000 (87%)]\tLoss: 0.521324\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 10 [1400/8000 (17%)]\tLoss: 0.323208\n",
      "Train Epoch: 10 [2800/8000 (35%)]\tLoss: 0.364322\n",
      "Train Epoch: 10 [4200/8000 (52%)]\tLoss: 0.486788\n",
      "Train Epoch: 10 [5600/8000 (70%)]\tLoss: 0.447607\n",
      "Train Epoch: 10 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 11 [1400/8000 (17%)]\tLoss: 0.323209\n",
      "Train Epoch: 11 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 11 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 11 [5600/8000 (70%)]\tLoss: 0.447607\n",
      "Train Epoch: 11 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 12 [1400/8000 (17%)]\tLoss: 0.323210\n",
      "Train Epoch: 12 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 12 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 12 [5600/8000 (70%)]\tLoss: 0.447607\n",
      "Train Epoch: 12 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 13 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 13 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 13 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 13 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 13 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 14 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 14 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 14 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 14 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 14 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 15 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 15 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 15 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 15 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 15 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 16 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 16 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 16 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 16 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 16 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 17 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 17 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 17 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 17 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 17 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 18 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 18 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 18 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 18 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 18 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 19 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 19 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 19 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 19 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 19 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 20 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 20 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 20 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 20 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 20 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 21 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 21 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 21 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 21 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 21 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 22 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 22 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 22 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 22 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 22 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 23 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 23 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 23 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 23 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 23 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.387669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 24 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 24 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 24 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 24 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 25 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 25 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 25 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 25 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 25 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 26 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 26 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 26 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 26 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 26 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 27 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 27 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 27 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 27 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 27 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 28 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 28 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 28 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 28 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 28 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 29 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 29 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 29 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 29 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 29 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 30 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 30 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 30 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 30 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 30 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 31 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 31 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 31 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 31 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 31 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 31 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 32 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 32 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 32 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 32 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 32 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 32 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 33 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 33 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 33 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 33 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 33 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 33 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 34 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 34 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 34 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 34 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 34 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 34 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 35 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 35 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 35 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 35 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 35 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 35 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 36 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 36 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 36 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 36 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 36 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 36 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 37 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 37 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 37 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 37 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 37 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 37 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 38 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 38 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 38 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 38 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 38 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 38 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 39 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 39 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 39 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 39 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 39 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 39 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 40 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 40 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 40 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 40 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 40 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 40 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 41 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 41 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 41 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 41 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 41 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 41 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 42 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 42 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 42 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 42 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 42 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 42 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 43 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 43 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 43 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 43 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 43 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 43 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 44 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 44 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 44 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 44 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 44 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 44 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 45 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 45 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 45 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 45 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 45 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 45 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 46 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 46 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 46 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 46 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 46 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 46 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 47 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 47 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 47 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 47 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 47 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 47 [7000/8000 (87%)]\tLoss: 0.521325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 48 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 48 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 48 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 48 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 48 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 48 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 49 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 49 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 49 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 49 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 49 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 49 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 50 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 50 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 50 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 50 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 50 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 50 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 51 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 51 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 51 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 51 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 51 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 51 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 52 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 52 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 52 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 52 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 52 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 52 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 53 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 53 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 53 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 53 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 53 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 53 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 54 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 54 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 54 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 54 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 54 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 54 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 55 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 55 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 55 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 55 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 55 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 55 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 56 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 56 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 56 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 56 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 56 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 56 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 57 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 57 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 57 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 57 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 57 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 57 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 58 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 58 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 58 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 58 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 58 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 58 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 59 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 59 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 59 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 59 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 59 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 59 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 60 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 60 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 60 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 60 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 60 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 60 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 61 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 61 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 61 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 61 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 61 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 61 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 62 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 62 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 62 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 62 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 62 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 62 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 63 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 63 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 63 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 63 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 63 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 63 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 64 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 64 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 64 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 64 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 64 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 64 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 65 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 65 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 65 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 65 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 65 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 65 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 66 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 66 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 66 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 66 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 66 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 66 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 67 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 67 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 67 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 67 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 67 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 67 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 68 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 68 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 68 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 68 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 68 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 68 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 69 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 69 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 69 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 69 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 69 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 69 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 70 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 70 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 70 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 70 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 70 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 70 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 71 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 71 [1400/8000 (17%)]\tLoss: 0.323211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 71 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 71 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 71 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 72 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 72 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 72 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 72 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 72 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 72 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 73 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 73 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 73 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 73 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 73 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 73 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 74 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 74 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 74 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 74 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 74 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 74 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 75 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 75 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 75 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 75 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 75 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 75 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 76 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 76 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 76 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 76 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 76 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 76 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 77 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 77 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 77 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 77 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 77 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 77 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 78 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 78 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 78 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 78 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 78 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 78 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 79 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 79 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 79 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 79 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 79 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 79 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 80 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 80 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 80 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 80 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 80 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 80 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 81 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 81 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 81 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 81 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 81 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 81 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 82 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 82 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 82 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 82 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 82 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 82 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 83 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 83 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 83 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 83 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 83 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 83 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 84 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 84 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 84 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 84 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 84 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 84 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 85 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 85 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 85 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 85 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 85 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 85 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 86 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 86 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 86 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 86 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 86 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 86 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 87 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 87 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 87 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 87 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 87 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 87 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 88 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 88 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 88 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 88 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 88 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 88 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 89 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 89 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 89 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 89 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 89 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 89 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 90 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 90 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 90 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 90 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 90 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 90 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 91 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 91 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 91 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 91 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 91 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 91 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 92 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 92 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 92 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 92 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 92 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 92 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 93 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 93 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 93 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 93 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 93 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 93 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 94 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 94 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 94 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 94 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 94 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 94 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 95 [0/8000 (0%)]\tLoss: 0.387669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 95 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 95 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 95 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 95 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 96 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 96 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 96 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 96 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 96 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 96 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 97 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 97 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 97 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 97 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 97 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 97 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 98 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 98 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 98 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 98 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 98 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 98 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n",
      "Train Epoch: 99 [0/8000 (0%)]\tLoss: 0.387669\n",
      "Train Epoch: 99 [1400/8000 (17%)]\tLoss: 0.323211\n",
      "Train Epoch: 99 [2800/8000 (35%)]\tLoss: 0.364323\n",
      "Train Epoch: 99 [4200/8000 (52%)]\tLoss: 0.486789\n",
      "Train Epoch: 99 [5600/8000 (70%)]\tLoss: 0.447608\n",
      "Train Epoch: 99 [7000/8000 (87%)]\tLoss: 0.521325\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 1584/2000 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch.customer_churn_prediction import main\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(batch_size=14, epochs=100, lr=0.01, input_dim=13, output_dim=2, data_dir='../data/Churn_Modelling.csv', test_size=0.2)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data transformation (make everything numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_numerical(data_df):\n",
    "    dataset_df = data_df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "    dataset_df = pd.get_dummies(dataset_df)\n",
    "    features = np.array(dataset_df.drop('Exited', axis=1))\n",
    "    feature_names = dataset_df.drop('Exited', axis=1).columns.tolist()\n",
    "    labels = np.array(dataset_df['Exited'])\n",
    "    \n",
    "    if len(features) > 1:\n",
    "        features = (features - features.mean(axis=0, keepdims=True)) / features.std(axis=0, keepdims=True)\n",
    "    \n",
    "    return features, feature_names, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.32622142,  0.29351742, -1.04175968, ..., -0.57380915,\n",
       "          1.09598752, -1.09598752],\n",
       "        [-0.44003595,  0.19816383, -1.38753759, ...,  1.74273971,\n",
       "          1.09598752, -1.09598752],\n",
       "        [-1.53679418,  0.29351742,  1.03290776, ..., -0.57380915,\n",
       "          1.09598752, -1.09598752],\n",
       "        ...,\n",
       "        [ 0.60498839, -0.27860412,  0.68712986, ..., -0.57380915,\n",
       "          1.09598752, -1.09598752],\n",
       "        [ 1.25683526,  0.29351742, -0.69598177, ..., -0.57380915,\n",
       "         -0.91241915,  0.91241915],\n",
       "        [ 1.46377078, -1.04143285, -0.35020386, ..., -0.57380915,\n",
       "          1.09598752, -1.09598752]]),\n",
       " ['CreditScore',\n",
       "  'Age',\n",
       "  'Tenure',\n",
       "  'Balance',\n",
       "  'NumOfProducts',\n",
       "  'HasCrCard',\n",
       "  'IsActiveMember',\n",
       "  'EstimatedSalary',\n",
       "  'Geography_France',\n",
       "  'Geography_Germany',\n",
       "  'Geography_Spain',\n",
       "  'Gender_Female',\n",
       "  'Gender_Male'],\n",
       " array([1, 0, 1, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, feature_names, labels = transform_to_numerical(data)\n",
    "features, feature_names, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class ChurnModellingDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None,):\n",
    "        assert features.shape[0] == labels.shape[0], \"The lengths of features and labels do not match\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.features = torch.tensor(features, dtype=torch.float)\n",
    "        self.labels = torch.tensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " (tensor([-0.3262,  0.2935, -1.0418, -1.2258, -0.9116,  0.6461,  0.9702,  0.0219,\n",
       "           0.9972, -0.5787, -0.5738,  1.0960, -1.0960]),\n",
       "  tensor(1)),\n",
       " 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ChurnModellingDataset(features, labels)\n",
    "len(dataset), dataset[0], len(dataset.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)*(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "lr_rate = 0.01\n",
    "epochs = 100\n",
    "input_dim = 13\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [8000, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000, torch.utils.data.dataset.Subset)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set), type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 40)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3. Create Model Class\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, target)\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8000 (0%)]\tLoss: 0.704951\n",
      "Train Epoch: 0 [5000/8000 (62%)]\tLoss: 0.472603\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1620/2000 (81%)\n",
      "\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.382423\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.481732\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1620/2000 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.381224\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.482112\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1617/2000 (81%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.381236\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.482254\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1617/2000 (81%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.381233\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.482345\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1617/2000 (81%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.381226\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.482408\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.381218\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.482453\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.381212\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.482487\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.381206\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.482513\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.381201\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.482534\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.381197\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.482550\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.381194\n",
      "Train Epoch: 11 [5000/8000 (62%)]\tLoss: 0.482563\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.381191\n",
      "Train Epoch: 12 [5000/8000 (62%)]\tLoss: 0.482573\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.381189\n",
      "Train Epoch: 13 [5000/8000 (62%)]\tLoss: 0.482582\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.381187\n",
      "Train Epoch: 14 [5000/8000 (62%)]\tLoss: 0.482589\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.381186\n",
      "Train Epoch: 15 [5000/8000 (62%)]\tLoss: 0.482595\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 16 [0/8000 (0%)]\tLoss: 0.381184\n",
      "Train Epoch: 16 [5000/8000 (62%)]\tLoss: 0.482600\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 17 [0/8000 (0%)]\tLoss: 0.381183\n",
      "Train Epoch: 17 [5000/8000 (62%)]\tLoss: 0.482604\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 18 [0/8000 (0%)]\tLoss: 0.381182\n",
      "Train Epoch: 18 [5000/8000 (62%)]\tLoss: 0.482607\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 19 [0/8000 (0%)]\tLoss: 0.381182\n",
      "Train Epoch: 19 [5000/8000 (62%)]\tLoss: 0.482610\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 20 [0/8000 (0%)]\tLoss: 0.381181\n",
      "Train Epoch: 20 [5000/8000 (62%)]\tLoss: 0.482612\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 21 [0/8000 (0%)]\tLoss: 0.381180\n",
      "Train Epoch: 21 [5000/8000 (62%)]\tLoss: 0.482614\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 22 [0/8000 (0%)]\tLoss: 0.381180\n",
      "Train Epoch: 22 [5000/8000 (62%)]\tLoss: 0.482616\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 23 [0/8000 (0%)]\tLoss: 0.381180\n",
      "Train Epoch: 23 [5000/8000 (62%)]\tLoss: 0.482618\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 24 [0/8000 (0%)]\tLoss: 0.381179\n",
      "Train Epoch: 24 [5000/8000 (62%)]\tLoss: 0.482619\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 25 [0/8000 (0%)]\tLoss: 0.381179\n",
      "Train Epoch: 25 [5000/8000 (62%)]\tLoss: 0.482620\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 26 [0/8000 (0%)]\tLoss: 0.381179\n",
      "Train Epoch: 26 [5000/8000 (62%)]\tLoss: 0.482621\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 27 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 27 [5000/8000 (62%)]\tLoss: 0.482621\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 28 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 28 [5000/8000 (62%)]\tLoss: 0.482622\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 29 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 29 [5000/8000 (62%)]\tLoss: 0.482623\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 30 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 30 [5000/8000 (62%)]\tLoss: 0.482623\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 31 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 31 [5000/8000 (62%)]\tLoss: 0.482623\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 32 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 32 [5000/8000 (62%)]\tLoss: 0.482624\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 33 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 33 [5000/8000 (62%)]\tLoss: 0.482624\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 34 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 34 [5000/8000 (62%)]\tLoss: 0.482624\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 35 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 35 [5000/8000 (62%)]\tLoss: 0.482624\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 36 [0/8000 (0%)]\tLoss: 0.381178\n",
      "Train Epoch: 36 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 37 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 37 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 38 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 38 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 39 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 39 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 40 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 40 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 41 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 41 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 42 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 42 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 43 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 43 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 44 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 44 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 45 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 45 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 46 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 46 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 47 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 47 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 48 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 48 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 49 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 49 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 50 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 50 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 51 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 51 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 52 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 52 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 53 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 53 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 54 [0/8000 (0%)]\tLoss: 0.381177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 54 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 55 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 55 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 56 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 56 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 57 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 57 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 58 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 58 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 59 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 59 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 60 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 60 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 61 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 61 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 62 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 62 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 63 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 63 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 64 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 64 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 65 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 65 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 66 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 66 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 67 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 67 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 68 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 68 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 69 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 69 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 70 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 70 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 71 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 71 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 72 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 72 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 73 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 73 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 74 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 74 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 75 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 75 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 76 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 76 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 77 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 77 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 78 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 78 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 79 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 79 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 80 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 80 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 81 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 81 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 82 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 82 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 83 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 83 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 84 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 84 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 85 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 85 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 86 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 86 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 87 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 87 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 88 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 88 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 89 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 89 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 90 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 90 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 91 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 91 [5000/8000 (62%)]\tLoss: 0.482625\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 92 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 92 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 93 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 93 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 94 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 94 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 95 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 95 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 96 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 96 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 97 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 97 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 98 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 98 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n",
      "Train Epoch: 99 [0/8000 (0%)]\tLoss: 0.381177\n",
      "Train Epoch: 99 [5000/8000 (62%)]\tLoss: 0.482626\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 1615/2000 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(13, 2)\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Step 7. Train Model\n",
    "for epoch in range(int(epochs)):\n",
    "    train(model, epoch, train_loader, criterion, optimizer)\n",
    "    test(model, None, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted a untrained example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12121</td>\n",
       "      <td>Any</td>\n",
       "      <td>600</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId Surname  CreditScore Geography Gender  Age  Tenure  \\\n",
       "0          1       12121     Any          600    France   Male   40       3   \n",
       "\n",
       "   Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \\\n",
       "0    60000              2          1               1            50000       1   \n",
       "\n",
       "   Geography_Germany  Geography_Spain  Gender_Female  \n",
       "0                  0                0              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"RowNumber\",\"CustomerId\",\"Surname\",\"CreditScore\",\"Geography\",\"Gender\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\",\"EstimatedSalary\",\"Exited\", 'Geography_Germany', 'Geography_Spain', 'Gender_Female']\n",
    "data = [[1, 12121, \"Any\", 600, \"France\", \"Male\", 40, 3, 60000, 2, 1, 1, 50000, 1, 0,0,0]]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df\n",
    "# record = np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])\n",
    "# record_label = np.array([0.0])\n",
    "# loader = transforms.Compose([transforms.Normalize((0.1307,), (0.3081,)), transforms.ToTensor()])\n",
    "# record = (record - record.mean(axis=0, keepdims=True)) / record.std(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CreditScore',\n",
       " 'Age',\n",
       " 'Tenure',\n",
       " 'Balance',\n",
       " 'NumOfProducts',\n",
       " 'HasCrCard',\n",
       " 'IsActiveMember',\n",
       " 'EstimatedSalary',\n",
       " 'Geography_France',\n",
       " 'Geography_Germany',\n",
       " 'Geography_Spain',\n",
       " 'Gender_Female',\n",
       " 'Gender_Male']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  600,    40,     3, 60000,     2,     1,     1, 50000,     0,\n",
       "             0,     0,     1,     1]]),\n",
       " ['CreditScore',\n",
       "  'Age',\n",
       "  'Tenure',\n",
       "  'Balance',\n",
       "  'NumOfProducts',\n",
       "  'HasCrCard',\n",
       "  'IsActiveMember',\n",
       "  'EstimatedSalary',\n",
       "  'Geography_Germany',\n",
       "  'Geography_Spain',\n",
       "  'Gender_Female',\n",
       "  'Geography_France',\n",
       "  'Gender_Male'],\n",
       " array([1]),\n",
       " 13)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record, record_features, record_label = transform_to_numerical(df)\n",
    "record, record_features, record_label, len(record_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.std(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChurnModellingDataset(record, record_label)\n",
    "len(dataset), dataset[0], len(dataset.get_feature_names())\n",
    "sample_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in sample_loader:\n",
    "    outputs = model(images)\n",
    "    pred = outputs.argmax(dim=1, keepdim=True) \n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20586.8164,  -3902.1030]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.kaggle.com/dragonoken/churn-modelling-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99. Loss: 0.45080384612083435. Accuracy: 80. epoch:0\n",
      "Iteration: 199. Loss: 0.636818528175354. Accuracy: 81. epoch:0\n",
      "Iteration: 299. Loss: 0.3298513889312744. Accuracy: 80. epoch:0\n",
      "Iteration: 399. Loss: 0.15808606147766113. Accuracy: 80. epoch:0\n",
      "Iteration: 499. Loss: 0.6422808170318604. Accuracy: 80. epoch:0\n",
      "Iteration: 599. Loss: 0.38669145107269287. Accuracy: 80. epoch:0\n",
      "Iteration: 699. Loss: 0.4859692454338074. Accuracy: 80. epoch:0\n",
      "Iteration: 799. Loss: 0.8278332948684692. Accuracy: 80. epoch:0\n",
      "Iteration: 99. Loss: 0.422950804233551. Accuracy: 81. epoch:1\n",
      "Iteration: 199. Loss: 0.6574752926826477. Accuracy: 81. epoch:1\n",
      "Iteration: 299. Loss: 0.32711881399154663. Accuracy: 80. epoch:1\n",
      "Iteration: 399. Loss: 0.1563369184732437. Accuracy: 80. epoch:1\n",
      "Iteration: 499. Loss: 0.642701268196106. Accuracy: 80. epoch:1\n",
      "Iteration: 599. Loss: 0.3862289786338806. Accuracy: 80. epoch:1\n",
      "Iteration: 699. Loss: 0.48686036467552185. Accuracy: 80. epoch:1\n",
      "Iteration: 799. Loss: 0.8279159665107727. Accuracy: 80. epoch:1\n",
      "Iteration: 99. Loss: 0.4228644371032715. Accuracy: 81. epoch:2\n",
      "Iteration: 199. Loss: 0.6577866673469543. Accuracy: 81. epoch:2\n",
      "Iteration: 299. Loss: 0.3272186815738678. Accuracy: 80. epoch:2\n",
      "Iteration: 399. Loss: 0.15617509186267853. Accuracy: 80. epoch:2\n",
      "Iteration: 499. Loss: 0.6427619457244873. Accuracy: 80. epoch:2\n",
      "Iteration: 599. Loss: 0.3861146569252014. Accuracy: 80. epoch:2\n",
      "Iteration: 699. Loss: 0.48708638548851013. Accuracy: 80. epoch:2\n",
      "Iteration: 799. Loss: 0.8279236555099487. Accuracy: 80. epoch:2\n",
      "Iteration: 99. Loss: 0.4228394031524658. Accuracy: 81. epoch:3\n",
      "Iteration: 199. Loss: 0.6578805446624756. Accuracy: 81. epoch:3\n",
      "Iteration: 299. Loss: 0.32725104689598083. Accuracy: 80. epoch:3\n",
      "Iteration: 399. Loss: 0.15611979365348816. Accuracy: 80. epoch:3\n",
      "Iteration: 499. Loss: 0.6427822113037109. Accuracy: 80. epoch:3\n",
      "Iteration: 599. Loss: 0.38607338070869446. Accuracy: 80. epoch:3\n",
      "Iteration: 699. Loss: 0.48716840147972107. Accuracy: 80. epoch:3\n",
      "Iteration: 799. Loss: 0.8279250264167786. Accuracy: 80. epoch:3\n",
      "Iteration: 99. Loss: 0.42282962799072266. Accuracy: 81. epoch:4\n",
      "Iteration: 199. Loss: 0.6579165458679199. Accuracy: 81. epoch:4\n",
      "Iteration: 299. Loss: 0.3272637724876404. Accuracy: 80. epoch:4\n",
      "Iteration: 399. Loss: 0.15609757602214813. Accuracy: 80. epoch:4\n",
      "Iteration: 499. Loss: 0.6427899599075317. Accuracy: 80. epoch:4\n",
      "Iteration: 599. Loss: 0.3860566318035126. Accuracy: 80. epoch:4\n",
      "Iteration: 699. Loss: 0.48720186948776245. Accuracy: 80. epoch:4\n",
      "Iteration: 799. Loss: 0.8279252052307129. Accuracy: 80. epoch:4\n",
      "Iteration: 99. Loss: 0.42282527685165405. Accuracy: 81. epoch:5\n",
      "Iteration: 199. Loss: 0.6579316854476929. Accuracy: 81. epoch:5\n",
      "Iteration: 299. Loss: 0.32726919651031494. Accuracy: 80. epoch:5\n",
      "Iteration: 399. Loss: 0.15608815848827362. Accuracy: 80. epoch:5\n",
      "Iteration: 499. Loss: 0.6427932977676392. Accuracy: 80. epoch:5\n",
      "Iteration: 599. Loss: 0.38604938983917236. Accuracy: 80. epoch:5\n",
      "Iteration: 699. Loss: 0.48721614480018616. Accuracy: 80. epoch:5\n",
      "Iteration: 799. Loss: 0.8279250264167786. Accuracy: 80. epoch:5\n",
      "Iteration: 99. Loss: 0.4228232800960541. Accuracy: 81. epoch:6\n",
      "Iteration: 199. Loss: 0.6579381227493286. Accuracy: 81. epoch:6\n",
      "Iteration: 299. Loss: 0.3272715210914612. Accuracy: 80. epoch:6\n",
      "Iteration: 399. Loss: 0.15608401596546173. Accuracy: 80. epoch:6\n",
      "Iteration: 499. Loss: 0.6427946090698242. Accuracy: 80. epoch:6\n",
      "Iteration: 599. Loss: 0.38604623079299927. Accuracy: 80. epoch:6\n",
      "Iteration: 699. Loss: 0.4872223436832428. Accuracy: 80. epoch:6\n",
      "Iteration: 799. Loss: 0.827924907207489. Accuracy: 80. epoch:6\n",
      "Iteration: 99. Loss: 0.42282238602638245. Accuracy: 81. epoch:7\n",
      "Iteration: 199. Loss: 0.6579412221908569. Accuracy: 81. epoch:7\n",
      "Iteration: 299. Loss: 0.32727259397506714. Accuracy: 80. epoch:7\n",
      "Iteration: 399. Loss: 0.15608224272727966. Accuracy: 80. epoch:7\n",
      "Iteration: 499. Loss: 0.642795205116272. Accuracy: 80. epoch:7\n",
      "Iteration: 599. Loss: 0.38604483008384705. Accuracy: 80. epoch:7\n",
      "Iteration: 699. Loss: 0.487225204706192. Accuracy: 80. epoch:7\n",
      "Iteration: 799. Loss: 0.827924907207489. Accuracy: 80. epoch:7\n",
      "Iteration: 99. Loss: 0.42282190918922424. Accuracy: 81. epoch:8\n",
      "Iteration: 199. Loss: 0.6579423546791077. Accuracy: 81. epoch:8\n",
      "Iteration: 299. Loss: 0.32727307081222534. Accuracy: 80. epoch:8\n",
      "Iteration: 399. Loss: 0.156081423163414. Accuracy: 80. epoch:8\n",
      "Iteration: 499. Loss: 0.6427954435348511. Accuracy: 80. epoch:8\n",
      "Iteration: 599. Loss: 0.3860441744327545. Accuracy: 80. epoch:8\n",
      "Iteration: 699. Loss: 0.4872263967990875. Accuracy: 80. epoch:8\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:8\n",
      "Iteration: 99. Loss: 0.42282170057296753. Accuracy: 81. epoch:9\n",
      "Iteration: 199. Loss: 0.6579429507255554. Accuracy: 81. epoch:9\n",
      "Iteration: 299. Loss: 0.32727324962615967. Accuracy: 80. epoch:9\n",
      "Iteration: 399. Loss: 0.15608109533786774. Accuracy: 80. epoch:9\n",
      "Iteration: 499. Loss: 0.6427955627441406. Accuracy: 80. epoch:9\n",
      "Iteration: 599. Loss: 0.38604387640953064. Accuracy: 80. epoch:9\n",
      "Iteration: 699. Loss: 0.4872269630432129. Accuracy: 80. epoch:9\n",
      "Iteration: 799. Loss: 0.8279248476028442. Accuracy: 80. epoch:9\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:10\n",
      "Iteration: 199. Loss: 0.6579432487487793. Accuracy: 81. epoch:10\n",
      "Iteration: 299. Loss: 0.32727333903312683. Accuracy: 80. epoch:10\n",
      "Iteration: 399. Loss: 0.15608088672161102. Accuracy: 80. epoch:10\n",
      "Iteration: 499. Loss: 0.6427955627441406. Accuracy: 80. epoch:10\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:10\n",
      "Iteration: 699. Loss: 0.487227201461792. Accuracy: 80. epoch:10\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:10\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:11\n",
      "Iteration: 199. Loss: 0.6579434275627136. Accuracy: 81. epoch:11\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:11\n",
      "Iteration: 399. Loss: 0.15608076751232147. Accuracy: 80. epoch:11\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:11\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:11\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:11\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:11\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:12\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:12\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:12\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:12\n",
      "Iteration: 499. Loss: 0.6427955627441406. Accuracy: 80. epoch:12\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:12\n",
      "Iteration: 699. Loss: 0.48722729086875916. Accuracy: 80. epoch:12\n",
      "Iteration: 799. Loss: 0.8279248476028442. Accuracy: 80. epoch:12\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:13\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:13\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:13\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:13\n",
      "Iteration: 499. Loss: 0.6427955627441406. Accuracy: 80. epoch:13\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:13\n",
      "Iteration: 699. Loss: 0.48722729086875916. Accuracy: 80. epoch:13\n",
      "Iteration: 799. Loss: 0.8279248476028442. Accuracy: 80. epoch:13\n",
      "Iteration: 99. Loss: 0.42282161116600037. Accuracy: 81. epoch:14\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:14\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:14\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:14\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:14\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:14\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:14\n",
      "Iteration: 799. Loss: 0.8279248476028442. Accuracy: 80. epoch:14\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:15\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:15\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:15\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:15\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:15\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:15\n",
      "Iteration: 699. Loss: 0.48722729086875916. Accuracy: 80. epoch:15\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:15\n",
      "Iteration: 99. Loss: 0.4228215217590332. Accuracy: 81. epoch:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:16\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:16\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:16\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:16\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:16\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:16\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:16\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:17\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:17\n",
      "Iteration: 299. Loss: 0.32727333903312683. Accuracy: 80. epoch:17\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:17\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:17\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:17\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:17\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:17\n",
      "Iteration: 99. Loss: 0.42282161116600037. Accuracy: 81. epoch:18\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:18\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:18\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:18\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:18\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:18\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:18\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:18\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:19\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:19\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:19\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:19\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:19\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:19\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:19\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:19\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:20\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:20\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:20\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:20\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:20\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:20\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:20\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:20\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:21\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:21\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:21\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:21\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:21\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:21\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:21\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:21\n",
      "Iteration: 99. Loss: 0.4228215217590332. Accuracy: 81. epoch:22\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:22\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:22\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:22\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:22\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:22\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:22\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:22\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:23\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:23\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:23\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:23\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:23\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:23\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:23\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:23\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:24\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:24\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:24\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:24\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:24\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:24\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:24\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:24\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:25\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:25\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:25\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:25\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:25\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:25\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:25\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:25\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:26\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:26\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:26\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:26\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:26\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:26\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:26\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:26\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:27\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:27\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:27\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:27\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:27\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:27\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:27\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:27\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:28\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:28\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:28\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:28\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:28\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:28\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:28\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:28\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:29\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:29\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:29\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:29\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:29\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:29\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:29\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:29\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:30\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:30\n",
      "Iteration: 299. Loss: 0.32727333903312683. Accuracy: 80. epoch:30\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:30\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:30\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:30\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:30\n",
      "Iteration: 799. Loss: 0.8279245495796204. Accuracy: 80. epoch:30\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:31\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:31\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:31\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:31\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:31\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:31\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:31\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:31\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:32\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:32\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:32\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:32\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:32\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:32\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:32\n",
      "Iteration: 99. Loss: 0.42282161116600037. Accuracy: 81. epoch:33\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:33\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:33\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:33\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:33\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:33\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:33\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:33\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:34\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:34\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:34\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:34\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:34\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:34\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:34\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:34\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:35\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:35\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:35\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:35\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:35\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:35\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:35\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:35\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:36\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:36\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:36\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:36\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:36\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:36\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:36\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:36\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:37\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:37\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:37\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:37\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:37\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:37\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:37\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:37\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:38\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:38\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:38\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:38\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:38\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:38\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:38\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:38\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:39\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:39\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:39\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:39\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:39\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:39\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:39\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:39\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:40\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:40\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:40\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:40\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:40\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:40\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:40\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:40\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:41\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:41\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:41\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:41\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:41\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:41\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:41\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:41\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:42\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:42\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:42\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:42\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:42\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:42\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:42\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:42\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:43\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:43\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:43\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:43\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:43\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:43\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:43\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:43\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:44\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:44\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:44\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:44\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:44\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:44\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:44\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:44\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:45\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:45\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:45\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:45\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:45\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:45\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:45\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:45\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:46\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:46\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:46\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:46\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:46\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:46\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:46\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:46\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:47\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:47\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:47\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:47\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:47\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:47\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:47\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:47\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:48\n",
      "Iteration: 299. Loss: 0.32727333903312683. Accuracy: 80. epoch:48\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:48\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:48\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:48\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:48\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:48\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:49\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:49\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:49\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:49\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:49\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:49\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:49\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:49\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:50\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:50\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:50\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:50\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:50\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:50\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:50\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:50\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:51\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:51\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:51\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:51\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:51\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:51\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:51\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:51\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:52\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:52\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:52\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:52\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:52\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:52\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:52\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:52\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:53\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:53\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:53\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:53\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:53\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:53\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:53\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:53\n",
      "Iteration: 99. Loss: 0.42282161116600037. Accuracy: 81. epoch:54\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:54\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:54\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:54\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:54\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:54\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:54\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:54\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:55\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:55\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:55\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:55\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:55\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:55\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:55\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:55\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:56\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:56\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:56\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:56\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:56\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:56\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:56\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:56\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:57\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:57\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:57\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:57\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:57\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:57\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:57\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:57\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:58\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:58\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:58\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:58\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:58\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:58\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:58\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:58\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:59\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:59\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:59\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:59\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:59\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:59\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:59\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:59\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:60\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:60\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:60\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:60\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:60\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:60\n",
      "Iteration: 699. Loss: 0.48722735047340393. Accuracy: 80. epoch:60\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:60\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:61\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:61\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:61\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:61\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:61\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:61\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:61\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:61\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:62\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:62\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:62\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:62\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:62\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:62\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:62\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:62\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:63\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:63\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:63\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:63\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:63\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:63\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:63\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:64\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:64\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:64\n",
      "Iteration: 399. Loss: 0.15608081221580505. Accuracy: 80. epoch:64\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:64\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:64\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:64\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:64\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:65\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:65\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:65\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:65\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:65\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:65\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:65\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:65\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:66\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:66\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:66\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:66\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:66\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:66\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:66\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:66\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:67\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:67\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:67\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:67\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:67\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:67\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:67\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:67\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:68\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:68\n",
      "Iteration: 299. Loss: 0.3272733986377716. Accuracy: 80. epoch:68\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:68\n",
      "Iteration: 499. Loss: 0.6427956223487854. Accuracy: 80. epoch:68\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:68\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:68\n",
      "Iteration: 799. Loss: 0.8279247283935547. Accuracy: 80. epoch:68\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:69\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:69\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:69\n",
      "Iteration: 399. Loss: 0.15608082711696625. Accuracy: 80. epoch:69\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:69\n",
      "Iteration: 599. Loss: 0.3860437870025635. Accuracy: 80. epoch:69\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:69\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:69\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:70\n",
      "Iteration: 199. Loss: 0.6579435467720032. Accuracy: 81. epoch:70\n",
      "Iteration: 299. Loss: 0.327273428440094. Accuracy: 80. epoch:70\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:70\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:70\n",
      "Iteration: 599. Loss: 0.3860437273979187. Accuracy: 80. epoch:70\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:70\n",
      "Iteration: 799. Loss: 0.8279246091842651. Accuracy: 80. epoch:70\n",
      "Iteration: 99. Loss: 0.422821581363678. Accuracy: 81. epoch:71\n",
      "Iteration: 199. Loss: 0.657943606376648. Accuracy: 81. epoch:71\n",
      "Iteration: 299. Loss: 0.3272733688354492. Accuracy: 80. epoch:71\n",
      "Iteration: 399. Loss: 0.15608084201812744. Accuracy: 80. epoch:71\n",
      "Iteration: 499. Loss: 0.6427956819534302. Accuracy: 80. epoch:71\n",
      "Iteration: 599. Loss: 0.3860437572002411. Accuracy: 80. epoch:71\n",
      "Iteration: 699. Loss: 0.4872273802757263. Accuracy: 80. epoch:71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-44cd7112f8b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6aaea5fb47e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############## DEPRECATED ##############\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# Step 7. Train Model\n",
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%100==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "#                 images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print(f\"Iteration: {i}. Loss: {loss.item()}. Accuracy: {accuracy}. epoch:{epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
